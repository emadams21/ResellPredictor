{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main Jupyter Note book that walks through the entire process of how I came about my Code.\n",
    "# I will not run these cells so they wont interfere with the notbooks I already set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing StockX Data Set into varible\n",
    "sneakerData = pd.read_csv('data/StockX-Data-Contest-2019-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning Cell\n",
    "\n",
    "#Making Sure all Sneaker Names are lowercase to be on the Same accord.\n",
    "sneakerData['Sneaker Name'] = sneakerData['Sneaker Name'].str.lower()\n",
    "\n",
    "#Cleaning the dates column\n",
    "sneakerData['Order Date'] = sneakerData['Order Date'].astype('datetime64[ns]')\n",
    "sneakerData['Release Date'] = sneakerData['Release Date'].astype('datetime64[ns]')\n",
    "sneakerData['Turnover Days'] = sneakerData['Order Date'] - sneakerData['Release Date']\n",
    "\n",
    "#Removing the $ and , From Sale Price and Retial Price to be able to turn into intergers for the future\n",
    "sneakerData['Sale Price'] =  sneakerData['Sale Price'].astype(str).str.replace('$', '')\n",
    "sneakerData['Sale Price'] =  sneakerData['Sale Price'].astype(str).str.replace(',', '')\n",
    "sneakerData['Retail Price'] =  sneakerData['Retail Price'].astype(str).str.replace('$', '')\n",
    "sneakerData['Retail Price'] =  sneakerData['Retail Price'].astype(str).str.replace(',', '')\n",
    "\n",
    "# Making Sales and Retial Price into Int\n",
    "sneakerData['Sale Price'] = sneakerData['Sale Price'].astype(int)\n",
    "sneakerData['Retail Price'] = sneakerData['Retail Price'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Profit Column to possibly compare how future features effects the profitability of the shoe.\n",
    "sneakerData['Profit'] = sneakerData['Sale Price'] - sneakerData['Retail Price']\n",
    "sneakerData['Profit Ratio'] = (sneakerData['Profit'] / sneakerData['Retail Price']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring and adding the Model Column to do Comparisons by Modeling.\n",
    "sneakerData[\"Model\"] = sneakerData['Sneaker Name'].apply(\n",
    "    lambda x : 'yeezy-boost-350' if 'yeezy' in x.split(\"-\") else (\n",
    "        'air-jordan-1-retro-high' if 'jordan' in x.split('-') else (\n",
    "            'air-force-1' if 'force' in x.split('-') else(\n",
    "                'air-max-90' if '90' in x.split('-') else (\n",
    "                    'air-max-97' if '97' in x.split('-') else (\n",
    "                        'air-presto' if 'presto' in x.split('-') else (\n",
    "                            'air-vapormax' if 'vapormax' in x.split('-') else (\n",
    "                                'blazer-mid' if 'blazer' in x.split('-') else (\n",
    "                                    'react-hyperdunk-2017-flyknit' if 'hyperdunk' in x.split('-') else (\n",
    "                                        'zoom-fly' if 'zoom' in x.split('-') else (np.nan)\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all the Sneaker Names are categorised into Model Variants\n",
    "uncategorised_model = pd.DataFrame()\n",
    "uncategorised_model[sneakerData['Model'].isnull() == True]\n",
    "print(uncategorised_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the colour information from stockX website, based on each sneaker name\n",
    "colourway = pd.read_excel('data/supplemental_data_colorway.xlsx')\n",
    "colourway['Style'] = colourway['Style'].str.lower()\n",
    "colourway.head()\n",
    "\n",
    "#merging the colourway DF with the sneakerData DF\n",
    "sneakerData = pd.merge(sneakerData,colourway,how='left',left_on='Sneaker Name',right_on='Style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Null values of Color columns with 0s to be able to self One-Hot-Encode Colors\n",
    "sneakerData['Black'] = sneakerData['Black'].fillna(0)\n",
    "sneakerData['White'] = sneakerData['White'].fillna(0)\n",
    "sneakerData['Green'] = sneakerData['Green'].fillna(0)\n",
    "sneakerData['Neo'] = sneakerData['Neo'].fillna(0)\n",
    "sneakerData['Orange'] = sneakerData['Orange'].fillna(0)\n",
    "sneakerData['Tan/Brown'] = sneakerData['Tan/Brown'].fillna(0)\n",
    "sneakerData['Pink'] = sneakerData['Pink'].fillna(0)\n",
    "sneakerData['Blue'] = sneakerData['Blue'].fillna(0)\n",
    "sneakerData['Colorful'] = sneakerData['Colorful'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Number of Sales, Website,Product Line,\n",
    "# and Buyer region to because I wanted to focus on the specific shoe features that could possibily be features.\n",
    "\n",
    "parsed_data = sneakerData.drop(['Number of Sales','Website','Product Line','Buyer Region'],axis=1)\n",
    "parsed_data['Turnover Days'] = parsed_data['Turnover Days'].dt.days\n",
    "parsed_data['Turnover Weeks'] = (parsed_data['Turnover Days'] / 7).round(0)\n",
    "parsed_data = parsed_data.drop(parsed_data[parsed_data['Turnover Weeks'] < 0].index)\n",
    "parsed_data = parsed_data.drop(parsed_data[parsed_data['Turnover Weeks'] > 52].index)\n",
    "\n",
    "#consolidating the primary colors into 1 single column\n",
    "def get_col(row):\n",
    "    for color in parsed_data.columns[10:20]:\n",
    "        if row[color] == 1:\n",
    "            return color\n",
    "parsed_data['color'] = parsed_data.apply(get_col, axis=1)\n",
    "parsed_data = parsed_data.replace(0, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Clean Data into new features.\n",
    "parsed_data.to_csv('parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA Visualization Preperation: This Series of steps are for preping the data so I can get better results when \n",
    "# displaying the data to find patterns and possible future features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Parsed Data Set from orignal Cleaning\n",
    "sneakerData = pd.read_csv('data/parsed_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing the Data Types in the Dataset\n",
    "sneakerData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on Feedback from mentors Ive decided that To help evaluate time to possibly be able to model I wanted to transfer The \n",
    "# Order Season and Release Season to represent actual dates to be able to code for seasons. \n",
    "\n",
    "#Putting into Dates Seperating Years, Months, and Days\n",
    "sneakerData['Order Date'] = pd.to_datetime(sneakerData['Order Date'],infer_datetime_format=True) \n",
    "sneakerData['Release Date'] = pd.to_datetime(sneakerData['Release Date'],infer_datetime_format=True)\n",
    "\n",
    "sneakerData['release_season'] = sneakerData['Release Date'].apply(lambda x: (x.month%12 + 3)//3)\n",
    "sneakerData['order_season'] = sneakerData['Order Date'].apply(lambda x: (x.month%12 + 3)//3)\n",
    "\n",
    "# Changing Dates to Seasons in addition to Years for ex: Fall 2019\n",
    "season_name = {1:'Winter', 2:'Spring', 3:\"Summer\",4:\"Fall\"}\n",
    "sneakerData['release_season'] = sneakerData['release_season'].map(season_name)\n",
    "sneakerData['order_season'] = sneakerData['order_season'].map(season_name)\n",
    "sneakerData['release_season'] = sneakerData['release_season'] + ' ' + sneakerData['release_year'].astype(str)\n",
    "\n",
    "#Adding just the Year to Release date and Order Date\n",
    "sneakerData['release_year'] = sneakerData['Release Date'].apply(lambda x: x.year)\n",
    "sneakerData['order_year'] = sneakerData['Order Date'].apply(lambda x: x.year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing Sale Price to Resell Price because that is what I am trying to measure.\n",
    "sneakerData = sneakerData.rename(columns={\"Sale Price\":\"Resell Price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Columns because I dont feel like they would be helpful to visualize\n",
    "sneakerData.drop(['Profit Ratio', 'color','Style','Turnover Days'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new dataframe\n",
    "sneakerData.to_csv('data/RegularShoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Is the Data Visualizaiton Piece of the Process to be able to see posisble features and to find interesting correlaitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function seperating Numerical Varibles and Categorical Variables\n",
    "def grab_columns(RegularShoes, cat_th=10, car_th = 20):\n",
    "    cat_col = [col for col in RegularShoes.columns if RegularShoes[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in RegularShoes.columns if RegularShoes[col].dtypes != \"O\" and RegularShoes[col].nunique() < cat_th]\n",
    "    cat_but_car = [col for col in RegularShoes.columns if RegularShoes[col].dtypes == \"O\" and RegularShoes[col].nunique() > car_th]\n",
    "    \n",
    "    cat_col = cat_col + num_but_cat\n",
    "    cat_col = [col for col in cat_col if col not in cat_but_car]\n",
    "    \n",
    "    num_col = [col for col in RegularShoes.columns if RegularShoes[col].dtypes != \"O\" and col not in num_but_cat]\n",
    "    \n",
    "    print(\"----- Categorical Columns -----\")\n",
    "    print(cat_col)\n",
    "    print(\"----- Numerical Columns -----\")\n",
    "    print(num_col)\n",
    "    print(\"----- Cardinal Columns -----\")\n",
    "    print(cat_but_car)\n",
    "    return cat_col,num_col,cat_but_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying Numerical Varibles and Categorical Variables \n",
    "cat_col, num_col,cat_but_car = grab_columns(RegularShoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to plot Numerical Analysis (Taken from Stack overflow will adjusted variables)\n",
    "\n",
    "def numerical_analysis(RegularShoes,col,plot=False):\n",
    "    print(\"Analysis For {}\".format(col))\n",
    "    quan = [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99]\n",
    "    print(RegularShoes[col].describe(quan))\n",
    "    if plot:\n",
    "        plt.figure(figsize= (10,10))\n",
    "        sns.histplot(RegularShoes[col],kde=True)\n",
    "        plt.title(col)\n",
    "        plt.xticks(rotation = 45)\n",
    "        plt.show(block=True)\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running that Analysis (Taken from Stack overflow will adjusted variables)\n",
    "for col in num_col:\n",
    "    numerical_analysis(RegularShoes,col,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to plot Categorical Analysis (Taken from Stack overflow will adjusted variables)\n",
    "def categorical_analysis(RegularShoes,col,plot=False):\n",
    "    print(\"Analysis For {}\".format(col))\n",
    "    print(pd.DataFrame({col:RegularShoes[col].value_counts(),\n",
    "                       \"Ratio\":100*RegularShoes[col].value_counts()/len(RegularShoes)}))\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.countplot(x=RegularShoes[col],data=RegularShoes)\n",
    "        plt.title(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show(block=True)\n",
    "        \n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running that Analysis (Taken from Stack overflow will adjusted variables)\n",
    "for col in cat_col:\n",
    "    categorical_analysis(RegularShoes,col,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help with feature finding I looked at a correlation matrix to see which posisble features correlate with eachother.\n",
    "correlations = RegularShoes.corr()\n",
    "sns.heatmap(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe\n",
    "RegularShoes.to_csv('data/RegularShoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that I found some Interesting Features I wanted to \n",
    "#do additional Data Cleaning once more to prepare the data to go through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading Data\n",
    "sneakerData = pd.read_csv('data/RegularShoes.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping Data Fram by Sneaker Name and Shoe Size to make it more logical for the model\n",
    "GroupedShoes = sneakerData.groupby(['Sneaker Name','Shoe Size'],as_index=False).agg({'Retail Price': 'min' , 'Sale Price': 'mean','Release Date': 'first','Order Date': 'first','Red':'first','Black':'first','White':'first','Green':'first','Neo':'first','Orange':'first','Tan/Brown':'first','Pink':'first','Blue':'first','Colorful':'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Data\n",
    "\n",
    "GroupedShoes.to_csv('data/GroupedShoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Onto Modeling the data into a series of simple models first to test then creating \n",
    "# a pipeline inorder to choose the best Model to run the data through then save the model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data \n",
    "GroupedShoes = pd.read_csv('data/GroupedShoes.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chosing X and Y. I chose Resell Price because that is what I want my model to predict.\n",
    "X = GroupedShoes.drop(['Resell Price',\"release_year\",\"order_year\",'Release Date','Order Date'], axis=1)\n",
    "y = GroupedShoes['Resell Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical data to numerical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "object_cols = ['Sneaker Name','release_season','order_season']\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# # Adding the column names after one hot encoding\n",
    "OH_cols_train.columns = OH_encoder.get_feature_names(object_cols)\n",
    "OH_cols_test.columns = OH_encoder.get_feature_names(object_cols)\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_test = X_test.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL TESTING\n",
    "#1. Linear Regression\n",
    "#2. OLS Regression\n",
    "# The first two models was only for simple testing to see what possible features my impact the model the most and \n",
    "#if the data ran into any overfitting or other errors.\n",
    "\n",
    "\n",
    "# Best Model Pipeline. I chose these models to test because I felt that they could give me the best results.\n",
    "#3. RandomForestRegressor\n",
    "#4. DecisionTreeRegressor\n",
    "#5. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "# Looking at y-int\n",
    "print(lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing predictions and running evaluation metrics. I chose these metrics because they can give me model accuracy and the \n",
    "#range it captured for Resell Prices because Resell Prices are volitle in a real situation.\n",
    "predictions = lm.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(\"MAE:\", metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS Regression\n",
    "def build_model(X,y):\n",
    "    X = sm.add_constant(X) #Adding the constant\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit() # fitting the model\n",
    "    print(results.summary()) # model summary\n",
    "    return X\n",
    "    \n",
    "def checkVIF(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running OLS Model\n",
    "X_train_new = build_model(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETING UP MODEL PIPELINE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipelines\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Decision Tree Regression Pipeline\n",
    "pipeline_dtr=Pipeline([('dtr', DecisionTreeRegressor(random_state=27))])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "pipeline_randomforest=Pipeline([('rf_regressor',RandomForestRegressor(random_state=27))])\n",
    "\n",
    "# XGBost Pipeline\n",
    "pipeline_xgb=Pipeline([('xgb_regressor',xgb.XGBRegressor(objective=\"reg:linear\", random_state=27))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of the pipelines to loop through them\n",
    "pipelines = [pipeline_dtr, pipeline_xgb, pipeline_randomforest]\n",
    "\n",
    "best_accuracy=0.0\n",
    "best_regressor=0\n",
    "best_pipeline=\"\"\n",
    "\n",
    "# Dictionary of pipelines and regression types for ease of reference\n",
    "pipe_dict = {0: 'DTR', 1: 'XGBoost', 2: 'RandomForest'}\n",
    "\n",
    "# Fit the pipelines\n",
    "for pipe in pipelines:\n",
    "\tpipe.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Checking the accuracy of each model\n",
    "for i,model in enumerate(pipelines):\n",
    "    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))\n",
    "\n",
    "# Finding the best model\n",
    "for i,model in enumerate(pipelines):\n",
    "    if model.score(X_test,y_test)>best_accuracy:\n",
    "        best_accuracy=model.score(X_test,y_test)\n",
    "        best_pipeline=model\n",
    "        best_regressor=i\n",
    "print('Model with best accuracy: {}'.format(pipe_dict[best_regressor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "#Random Forest Regresson\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Randomized Search CV to find the best parameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "# max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 25, 50, 75, 100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(random_state=27)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=27, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Random Search\n",
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    errors = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    print('Model Performance')\n",
    "    print('MSE of: ', errors)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Results\n",
    "from sklearn.metrics import mean_squared_error\n",
    "base_model = rf\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "best_random.fit(X_train , y_train)\n",
    "\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "\n",
    "print('\\n')\n",
    "print('Base Model Error: ', base_accuracy)\n",
    "print('\\n')\n",
    "print('Improved Model Error: ', random_accuracy)\n",
    "print('Improvement of {:0.2f}%.'.format((random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "print('\\n')\n",
    "print('RF_Randomized_Search_CV is complete.')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Random Forest Parameters and Score\n",
    "print('The best model is',rf_random.best_estimator_)\n",
    "print( rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing Cross Evaluation and printing MAE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(best_random, X_test, y_test,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "print(\"Average MAE Resell Price (across experiments):\\n\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model to disk\n",
    "import pickle\n",
    "pickle.dump(best_random, open('data/Resellmodel.pkl','wb'))\n",
    "\n",
    "# Loading model to compare the results\n",
    "model = pickle.load(open('data/Resellmodel.pkl','rb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
